@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

 @techreport{c2pa,
  author      = "{Coalition for Content Provenance and Authenticity}",
  title       = "Technical Specification 1.3",
  institution = "C2PA",
  year        = "2023",
  url = "https://c2pa.org/specifications/specifications/1.3/specs/_attachments/C2PA_Specification.pdf"
}
@misc{witness,
  author      = "S. Gregory",
  title       = "Ticks or it didn't happen",
  howpublished = "\url{https://lab.witness.org/ticks-or-it-didnt-happen/}",
  year        = "2019",
  note = {Accessed: 2024-01-20}
}

@inproceedings{icn,
  title={Deep Image Comparator: Learning to Visualize Editorial Change},
  author={Black, Alexander and Bui, Tu and Jin, Hailin and Swaminathan, Vishy and Collomosse, John},
  booktitle={Proc. CVPR WS},
  pages={972--980},
  year={2021},
  organization={IEEE}
}
@inproceedings{vpn,
  title={Vpn: Video provenance network for robust content attribution},
  author={Black, Alexander and Bui, Tu and Jenni, Simon and Swaminathan, Viswanathan and Collomosse, John},
  booktitle={Proc. CVMP},
  pages={1--10},
  year={2021}
}

@misc{relatt,
      title={Expressing Visual Relationships via Language}, 
      author={Hao Tan and Franck Dernoncourt and Zhe Lin and Trung Bui and Mohit Bansal},
      year={2019},
      eprint={1906.07689},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{psBattles,
  author        = {S. Heller and L. Rossetto and H. Schuldt},
  title         = {{The PS-Battles Dataset -- an Image Collection for Image Manipulation Detection}},
  journal       = {CoRR},
  volume        = {abs/1804.04866},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.04866},
  archivePrefix = {arXiv},
  eprint        = {1804.04866}
}
@inproceedings{oscarnet,
  title={Oscar-net: Object-centric scene graph attention for image attribution},
  author={Nguyen, Eric and Bui, Tu and Swaminathan, Viswanathan and Collomosse, John},
  booktitle={Proc. ICCV},
  pages={14499--14508},
  year={2021}
}
@inproceedings{pizzi,
  title={A self-supervised descriptor for image copy detection},
  author={Pizzi, Ed and Roy, Sreya Dutta and Ravindra, Sugosh Nagavara and Goyal, Priya and Douze, Matthijs},
  booktitle={Proc. CVPR},
  pages={14532--14542},
  year={2022}
}
@inproceedings{CLIP4IDC,
  title={CLIP4IDC: CLIP for Image Difference Captioning},
  author={Guo, Zixin and Wang, Tzu-Jui and Laaksonen, Jorma},
  booktitle={Proc. Conf. Asia-Pacific Chapter Assoc. Comp. Linguistics and Int. Joint Conf. NLP},
  pages={33--42},
  year={2022}
}
@inproceedings{IDC,
  title={Image difference captioning with pre-training and contrastive learning},
  author={Yao, Linli and Wang, Weiying and Jin, Qin},
  booktitle={Proc. AAAI},
  volume={36},
  pages={3108--3116},
  year={2022}
}

@ARTICLE{IFDC,
  author={Huang, Qingbao and Liang, Yu and Wei, Jielong and Cai, Yi and Liang, Hanyu and Leung, Ho-fung and Li, Qing},
  journal={IEEE Transactions on Multimedia}, 
  title={Image Difference Captioning With Instance-Level Fine-Grained Feature Representation}, 
  year={2022},
  volume={24},
  number={},
  pages={2004-2017},
  keywords={Feature extraction;Semantics;Visualization;Task analysis;Image color analysis;Proposals;Interference;Image difference captioning;change captioning;change description;instance-level;fine-grained feature extraction;similarity-based difference finding},
  doi={10.1109/TMM.2021.3074803}}

@inproceedings{R3Net,
    title = "{R}{\^{}}3{N}et:Relation-embedded Representation Reconstruction Network for Change Captioning",
    author = "Tu, Yunbin  and
      Li, Liang  and
      Yan, Chenggang  and
      Gao, Shengxiang  and
      Yu, Zhengtao",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.735",
    doi = "10.18653/v1/2021.emnlp-main.735",
    pages = "9319--9329",
    abstract = "Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In this paper, we propose a Relation-embedded Representation Reconstruction Network (R{\^{}}3Net) to explicitly distinguish the real change from the large amount of clutter and irrelevant changes. Specifically, a relation-embedded module is first devised to explore potential changed objects in the large amount of clutter. Then, based on the semantic similarities of corresponding locations in the two images, a representation reconstruction module (RRM) is designed to learn the reconstruction representation and further model the difference representation. Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the semantic interaction between change localization and caption generation. Extensive experiments show that the proposed method achieves the state-of-the-art results on two public datasets.",
}

@article{SGCC,
  title={Fully Convolutional CaptionNet: Siamese Difference Captioning Attention Model},
  author={Ariyo Oluwasanmi and Enoch Frimpong and Muhammad Umar Aftab and Edward Yellakuor Baagyere and Zhiguang Qin and Kifayat Ullah},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={175929-175939},
  url={https://api.semanticscholar.org/CorpusID:209382557}
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{NCT,
      title={Neighborhood Contrastive Transformer for Change Captioning}, 
      author={Yunbin Tu and Liang Li and Li Su and Ke Lu and Qingming Huang},
      year={2023},
      eprint={2303.03171},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{sem,
  title={Semantic Relation-aware Difference Representation Learning for Change Captioning},
  author={Tu, Yunbin and Yao, Tingting and Li, Liang and Lou, Jiedong and Gao, Shengxiang and Yu, Zhengtao and Yan, Chenggang},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={63--73},
  year={2021}
}

@inproceedings{M-VAM,
  title={Finding it at another side: A viewpoint-adapted matching encoder for change captioning},
  author={Shi, Xiangxi and Yang, Xu and Gu, Jiuxiang and Joty, Shafiq and Cai, Jianfei},
  booktitle={Proc. ECCV},
  pages={574--590},
  year={2020},
  organization={Springer}
}
@inproceedings{DUDA,
  title={Robust change captioning},
  author={Park, Dong Huk and Darrell, Trevor and Rohrbach, Anna},
  booktitle={Proc. ICCV},
  pages={4624--4633},
  year={2019}
}

@misc{gpt-j,
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2021},
  title={GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  note = {Accessed: 2024-01-20}
}


@article{ip2p,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  journal={arXiv preprint arXiv:2211.09800},
  year={2022}
}
@article{p2p,
  title={Prompt-to-prompt image editing with cross attention control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01626},
  year={2022}
}
@article{MAGMA,
  title={MAGMA--Multimodal Augmentation of Generative Models through Adapter-based Finetuning},
  author={Eichenberg, Constantin and Black, Sidney and Weinbach, Samuel and Parcalabescu, Letitia and Frank, Anette},
  journal={arXiv preprint arXiv:2112.05253},
  year={2021}
}
@article{LIMBER,
  title={Linearly mapping from image to text space},
  author={Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2209.15162},
  year={2022}
}
@inproceedings{spot-the-diff,
  title={Learning to Describe Differences Between Pairs of Similar Images},
  author={Jhamtani, Harsh and Berg-Kirkpatrick, Taylor},
  booktitle={Proc. Conf. Empirical Methods NLP},
  pages={4024--4034},
  year={2018}
}
@inproceedings{clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proc. CVPR},
  pages={2901--2910},
  year={2017}
}

@inproceedings{bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proc. Assoc. Comp. Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proc. ACL WS Intr. Extr. Eval. Measures Machine Trans. Summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proc. CVPR},
  pages={4566--4575},
  year={2015}
}

@inproceedings{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proc. ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR},
  url = {https://github.com/OpenAI/CLIP.},
}
@article{gpt-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{frozen,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={NeurIPS},
  volume={34},
  pages={200--212},
  year={2021}
}
@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proc. CVPR},
  pages={3128--3137},
  year={2015}
}
@inproceedings{donahue2015long,
  title={Long-term recurrent convolutional networks for visual recognition and description},
  author={Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  booktitle={Proc. CVPR},
  pages={2625--2634},
  year={2015}
}
@inproceedings{rennie2017self,
  title={Self-critical sequence training for image captioning},
  author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={Proc. CVPR},
  pages={7008--7024},
  year={2017}
}
@inproceedings{lu2017knowing,
  title={Knowing when to look: Adaptive attention via a visual sentinel for image captioning},
  author={Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  booktitle={Proc. CVPR},
  pages={375--383},
  year={2017}
}
@inproceedings{gu2018stack,
  title={Stack-captioning: Coarse-to-fine learning for image captioning},
  author={Gu, Jiuxiang and Cai, Jianfei and Wang, Gang and Chen, Tsuhan},
  booktitle={Proc. AAAI},
  volume={32},
  year={2018}
}
@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proc. CVPR},
  pages={6077--6086},
  year={2018}
}
@article{huang2019adaptively,
  title={Adaptively aligned image captioning via adaptive attention time},
  author={Huang, Lun and Wang, Wenmin and Xia, Yaxian and Chen, Jie},
  journal={NeurIPS},
  volume={32},
  year={2019}
}
@inproceedings{ge2019exploring,
  title={Exploring overall contextual information for image captioning in human-like cognitive style},
  author={Ge, Hongwei and Yan, Zehang and Zhang, Kai and Zhao, Mingde and Sun, Liang},
  booktitle={Proc. ICCV},
  pages={1754--1763},
  year={2019}
}
@inproceedings{yang2019auto,
  title={Auto-encoding scene graphs for image captioning},
  author={Yang, Xu and Tang, Kaihua and Zhang, Hanwang and Cai, Jianfei},
  booktitle={Proc. CVPR},
  pages={10685--10694},
  year={2019}
}
@inproceedings{yao2019hierarchy,
  title={Hierarchy parsing for image captioning},
  author={Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  booktitle={Proc. ICCV},
  pages={2621--2629},
  year={2019}
}
@inproceedings{cornia2020meshed,
  title={Meshed-memory transformer for image captioning},
  author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proc. CVPR},
  pages={10578--10587},
  year={2020}
}
@article{graves2012long,
  title={Long short-term memory},
  author={Graves, Alex and Graves, Alex},
  journal={Supervised sequence labelling with recurrent neural networks},
  pages={37--45},
  year={2012},
  publisher={Springer}
}
@inproceedings{vinyals2015show,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={Proc. CVPR},
  pages={3156--3164},
  year={2015}
}
@inproceedings{mao2014deep,
  title={Deep captioning with multimodal recurrent neural networks (m-rnn)},
  author={Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
  booktitle={Proc. ICLR},
  year={2015}
}
@inproceedings{pedersoli2017areas,
  title={Areas of attention for image captioning},
  author={Pedersoli, Marco and Lucas, Thomas and Schmid, Cordelia and Verbeek, Jakob},
  booktitle={Proc. ICCV},
  pages={1242--1250},
  year={2017}
}
@inproceedings{luo2021dual,
  title={Dual-level collaborative transformer for image captioning},
  author={Luo, Yunpeng and Ji, Jiayi and Sun, Xiaoshuai and Cao, Liujuan and Wu, Yongjian and Huang, Feiyue and Lin, Chia-Wen and Ji, Rongrong},
  booktitle={Proc. AAAI},
  volume={35},
  pages={2286--2293},
  year={2021}
}
@inproceedings{wang2021simvlm,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  booktitle={Proc. ICLR},
  year={2021}
}
@article{stefanini2022show,
  title={From show to tell: A survey on deep learning-based image captioning},
  author={Stefanini, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Cascianelli, Silvia and Fiameni, Giuseppe and Cucchiara, Rita},
  journal={IEEE TPAMI},
  volume={45},
  number={1},
  pages={539--559},
  year={2022},
  publisher={IEEE}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{hu2022scaling,
  title={Scaling up vision-language pre-training for image captioning},
  author={Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Yang, Zhengyuan and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={Proc. CVPR},
  pages={17980--17989},
  year={2022}
}
@inproceedings{zhang2021vinvl,
  title={Vinvl: Revisiting visual representations in vision-language models},
  author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle={Proc. CVPR},
  pages={5579--5588},
  year={2021}
}
@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={Proc. ECCV},
  pages={121--137},
  year={2020},
  organization={Springer}
}
@article{gao2023llama,
  title={LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}
@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{mokady2021clipcap,
  title={Clipcap: Clip prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}
@inproceedings{limoe,
  title={Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},
  author={Mustafa, Basil and Ruiz, Carlos Riquelme and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  booktitle={NeurIPS},
  year={2022}
}
@inproceedings{sd,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proc. CVPR},
  pages={10684--10695},
  year={2022}
}
@inproceedings{palme,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
    booktitle={arXiv preprint arXiv:2303.03378},
    year={2023}
}
@article{flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  volume={35},
  pages={23716--23736},
  year={2022}
}
@article{blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}
@inproceedings{kim2021agnostic,
  title={Agnostic change captioning with cycle consistency},
  author={Kim, Hoeseong and Kim, Jongseok and Lee, Hyungseok and Park, Hyunsung and Kim, Gunhee},
  booktitle={Proc. ICCV},
  pages={2095--2104},
  year={2021}
}
@article{sun2022bidirectional,
  title={Bidirectional difference locating and semantic consistency reasoning for change captioning},
  author={Sun, Yaoqi and Li, Liang and Yao, Tingting and Lu, Tongyv and Zheng, Bolun and Yan, Chenggang and Zhang, Hua and Bao, Yongjun and Ding, Guiguang and Slabaugh, Gregory},
  journal={IJIS},
  volume={37},
  number={5},
  pages={2969--2987},
  year={2022},
  publisher={Wiley Online Library}
}
@article{MPNet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={NeurIPS},
  volume={33},
  pages={16857--16867},
  year={2020}
}

@inproceedings{laion,
 author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
 booktitle = {NeurIPS},
 pages = {25278--25294},
 title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a1859debfb3b59d094f3504d5ebb6c25-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}

@article{gradcam,
	doi = {10.1007/s11263-019-01228-7},
	url = {https://doi.org/10.1007\%2Fs11263-019-01228-7},
	year = 2019,
	volume = {128},
	number = {2},
	pages = {336--359},
	author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
	journal = {International Journal of Computer Vision}
}

@ARTICLE{VARD,
  author={Tu, Yunbin and Li, Liang and Su, Li and Du, Junping and Lu, Ke and Huang, Qingming},
  journal={IEEE Transactions on Image Processing}, 
  title={Viewpoint-Adaptive Representation Disentanglement Network for Change Captioning}, 
  year={2023},
  volume={32},
  number={},
  pages={2620-2635},
  doi={10.1109/TIP.2023.3268004}}

@article{magicbrush,
  title={Magicbrush: A manually annotated dataset for instruction-guided image editing},
  author={Zhang, Kai and Mo, Lingbo and Chen, Wenhu and Sun, Huan and Su, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{clevr-change,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@inproceedings{augly,
  title={AugLy: Data augmentations for adversarial robustness},
  author={Papakipos, Zo{\"e} and Bitton, Joanna},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={156--163},
  year={2022}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{minigpt,
      title={MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning}, 
      author={Jun Chen and Deyao Zhu and Xiaoqian Shen and Xiang Li and Zechun Liu and Pengchuan Zhang and Raghuraman Krishnamoorthi and Vikas Chandra and Yunyang Xiong and Mohamed Elhoseiny},
      year={2023},
      eprint={2310.09478},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vit,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vixen,
      title={VIXEN: Visual Text Comparison Network for Image Difference Captioning}, 
      author={Alexander Black and Jing Shi and Yifei Fai and Tu Bui and John Collomosse},
      year={2024},
      eprint={2402.19119},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{h1,
      title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}, 
      author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
      journal={arXiv preprint arXiv:2309.01219},
      year={2023}
}

@article{h2,
   title={Survey of Hallucination in Natural Language Generation},
   volume={55},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3571730},
   DOI={10.1145/3571730},
   number={12},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
   year={2023},
   month=mar, pages={1–38} }
