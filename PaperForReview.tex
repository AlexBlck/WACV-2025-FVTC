% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{*****} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\LaTeX\ Author Guidelines for \confName~Proceedings}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
     We present FVTC -- a technique for image difference captioning that is
     able to benefit from additional visual and/or textual inputs. FVTC is able 
     to succinctly summarize multiple manipulations that were applied to an image
     in a sequence. Optionally, it can take several intermediate thumbnails of the
     image editing sequence as input, as well as coarse machine-generated annotations
     of the individual manipulations. We demonstrate that the presence of intermediate
     images and/or auxiliary textual information improves the model's captioning
     performance. To train FVTC, we introduce METS -- a new dataset of image editing sequences,
     with textual machine annotations of each editorial step and human edit summarization
     captions after the 5th, 10th and 15th manipulation.
\end{abstract}

\section{Introduction}



With the recent advancements of Generative AI, image manipulation becomes increasingly easier to perform and harder to notice. Often, multiple edits are applied in sequence by one or multiple editors, forming a provenance graph containing multiple versions of the same image at different stages of the editing process. To avoid the spread of misinformation, it is important to be able to communicate the history of these changes to the end user succinctly to enable them to make informed trust decisions \cite{witness}.

Image difference captioning (IDC) usually aims to generate a difference caption given two images, 
the original and the edited one, regardless of the number of manipulations applied to the image.
In this work, we explore image difference captioning with multiple inputs (IDC-MI),
assuming access to multiple snapshots of the image editing sequence and/or auxiliary information about each individual edit.
Such a setting may arise in the context of collaborative editing, where multiple editors contribute to the final image;
processing of a photo editing software history, or parsing of a provenance graph, where multiple versions of the same image are stored at different stages of the editing process.

The first challenge in edit sequence captioning is the limited availability of training data. Most datasets for image difference captioning focus on image pairs rather than longer sequences. While the Magic Brush \cite{magicbrush} dataset does provide multi-turn editing sequences, they are limited to three steps at most. Furthermore, all of the edits are applied to different non-overlapping objects, meaning that the final summary of all the manipulations could be constructed from a concatenation of the description of the individual steps. However, in real scenarios, the edits can be applied to the same area, potentially in a destructive or mutually exclusive manner, and the final summary should only describe the salient, still visible changes. For example, suppose the first manipulation changes the color of a bicycle, and the second one replaces the bicycle with a car. In that case, the final summary should not mention the color change as it is irrelevant to the final result. The second challenge lies in developing a methodology capable of handling interleaved multi-modal inputs. Many existing image difference captioning architectures are designed with exactly two image inputs in mind and would not be able to scale beyond that, either due to architectural constraints or memory limitations. The contributions of this paper are twofold:
% The contributions of this paper are twofold:
\begin{enumerate}
    \item First, we introduce METS (Multiple Edits and Textual Summaries) -- a dataset of image editing sequences, with textual machine annotations of each editorial step and human edit summarization captions after the 5th, 10th, and 15th manipulation.
    \item We train FVTC (Fusing Visual and Textual Cues) -- a multi-modal LLM with multiple visual inputs and provide a comprehensive evaluation of the benefits of both additional visual and textual inputs. 
\end{enumerate}

We demonstrate that the presence of intermediate images and/or auxiliary textual information improves the model's captioning performance. Additionally, we demonstrate that fine-tuning a model trained on other synthetic data with METS helps to bridge the domain gap and improves zero-shot performance on real-life images. The illustration of FVTC and METS is shown in Fig. \ref{fig:teaser}.



\section{Related Work}

Image difference captioning (IDC) is closely related to image captioning and visual question answering, both requiring a visual understanding system to model images and a language understanding system capable of generating syntactically correct captions. The revolution of IDC in recent years depends heavily on the advent of visual and text modeling approaches, together with cross-domain learning techniques that bridge the representation gap between them.

Initial methodologies for modeling visual content involve incorporating overarching CNN features such as VGG \cite{donahue2015long}, and ResNet \cite{rennie2017self} into text generation models. This integration capitalizes on the dense and meaningful representations these models provide. To enhance the representation of multiple objects and their interrelations, various techniques have emerged. Some methods \cite{lu2017knowing,gu2018stack,anderson2018bottom,huang2019adaptively}, partition images into discrete patches, extracting CNN features from each. Conversely, certain methodologies opt to utilize the outputs from an early ResNet layer, effectively capturing spatial attributes in a gridded format. In contrast, \cite{cornia2020meshed,anderson2018bottom,huang2019adaptively} employ Region Proposal Network (RPN) to extract features from potential object candidates, thereby improving alignment with the semantic entities referenced in paired captions. Other avenues of exploration include graph-based \cite{yang2019auto} and tree-based networks \cite{yao2019hierarchy}, aiming to capture object relations across varying levels of granularity.

% [global, grid, regional; CLIP \cite{CLIP} Vit ]\\

Traditionally, RNN/LSTM architectures \cite{graves2012long} have dominated text modeling owing to their intrinsic sequential nature. Variants like single-layer RNN \cite{vinyals2015show,mao2014deep} or double-layer LSTM \cite{donahue2015long,anderson2018bottom,yao2019hierarchy} are commonly utilized, often coupled with diverse methods to embed image features more deeply into the recurrent process, such as additive attention \cite{stefanini2022show}. During inference, captions are generated in a step-by-step manner, where the prediction of each word depends on all preceding words. Although this enhances linguistic coherence, RNN/LSTM-based approaches face challenges in modeling lengthy captions. Recent transformer-based methodologies, like those employing a full-attention mechanism \cite{luo2021dual,wang2021simvlm,cornia2020meshed}, have alleviated this issue. Advanced transformer-based models such as BERT \cite{devlin2018bert}, GPT \cite{gpt-3}, and LLaMA \cite{llama} have demonstrated success across diverse visual-language tasks \cite{hu2022scaling,mokady2021clipcap,gao2023llama,zhang2021vinvl,li2020oscar}.  

% [recurrent, attention, bert-based], GPT-3, GPT-J \cite{gpt-3, gpt-j} llama \\

The objective of visual language modeling is to establish connections between image/video and text representations, catering to specific tasks like joint embedding (e.g., CLIP \cite{CLIP} and LIMoE \cite{limoe} for cross-domain retrieval), text-to-image tasks (e.g., Stable Diffusion \cite{sd} for text-based image generation, InstructPix2Pix \cite{ip2p} for image editing), and image-to-text tasks (e.g., visual question answering \cite{flamingo,wang2021simvlm}, visual instructions \cite{gao2023llama,palme}). In the realm of image captioning, strategies for mapping images to text can be classified into two main approaches. The first approach involves the early fusion of image and text features to enhance alignment between image objects and textual descriptions \cite{frozen,mokady2021clipcap,wang2021simvlm,li2020oscar}. These methods employ BERT-like training strategies, where a pair of images and a masked caption are inputted, replacing the masked words during inference with either a start token or a prefixed phrase like 'A picture of'. The second approach centers on learning a direct conversion from image to text embedding. Initial CNN-based methods incorporate image features as the hidden states of LSTM text modules \cite{donahue2015long,vinyals2015show,yao2019hierarchy,karpathy2015deep,rennie2017self}, whereas later transformer-based techniques favor cross-attention mechanisms \cite{luo2021dual,cornia2020meshed}. Notably, recent trends in both approaches involve harnessing powerful pretrained large language and vision models to establish a straightforward mapping between the two domains \cite{LIMBER,MAGMA,blip2,frozen,mokady2021clipcap,minigpt}.
% Our proposed method also adopts this trend due to its 2-fold advantages: (i) it frees our model from learning domain-specific representations and (ii) enables VIXEN to be trained with limited/synthesis data without degrading its generalization capability.

Image difference captioning represents a specialized form of image captioning, aiming to disregard common objects across images and instead accentuate subtle alterations between them. Pioneering this domain, Spot-the-Diff \cite{spot-the-diff} introduces potential change clusters, employing an LSTM-based network to model them. However, their approach relies on pixel-level differences between input images, rendering it sensitive to noise and geometric transformations. In contrast, DUDA \cite{DUDA} computes image differences at the semantic level using CNNs, enhancing robustness against minor global alterations. Several approaches extend upon the foundation laid by DUDA. For example, SRDRL+AVS \cite{sem} initially assesses the correlation between the subtracted difference and image pairs to ascertain the occurrence of the change. Subsequently, it incorporates part-of-speech information to dynamically leverage visual data. M-VAM \cite{M-VAM} and VACC \cite{kim2021agnostic} propose a viewpoint encoder to mitigate viewpoint disparities, while VARD \cite{VARD} suggests a viewpoint invariant representation network to explicitly capture changes. Additionally, \cite{sun2022bidirectional} integrates bidirectional encoding to refine change localization, and NCT \cite{NCT} utilizes a transformer to aggregate neighboring features. These methodologies primarily concentrate on the image modality, exploiting benchmark-specific characteristics such as nearly identical views in Spot-the-Diff \cite{spot-the-diff} or synthetic scenes with limited objects and change types (color, texture, addition, deletion) in CLEVR \cite{DUDA}. More recently, IDC-PCL \cite{IDC} and CLIP4IDC \cite{CLIP4IDC} have adopted BERT-like training approaches to model difference captioning language, achieving state-of-the-art performance.

\section{Methodology}
\label{sec:method}

In this section, we describe the methodology behind the dataset generation as well as IDC model training.
Subsection \ref{sec:data} describes the data generation process used to create
the METS dataset. Subsection \ref{sec:arch} describes the architecture 
of the model used to train on the METS dataset to perform the multi-input image
difference captioning task.


\subsection{Data generation}
\label{sec:data}

We generate a dataset of image editing sequences, with textual machine annotations of each editorial step and human edit 
summarization captions after the 5th, 10th, and 15th manipulation, as shown in Fig.~\ref{fig:sequence}. Binary masks of the manipulation regions at each step are also included.
Our dataset covers a wide variety of pixel-level and generative manipulations. The prompt for each manipulation is generated using GPT-3.5 to ensure plausible and diverse manipulations.

\subsubsection{Individual Edits}

We identify two main categories of edits: pixel-level and generative
manipulations. Pixel-level edits are simple manipulations such as changing the
brightness of an image or applying a blur filter. Generative manipulations change
the semantic content of the image, altering the story that the image tells.

Pixel level manipulations are performed using the Augly\cite{augly} image
augmentation library, with a random choice of augmentation type and parameters.
Augmentation types include brightness, contrast, saturation, and encoding
quality changes; blur, noise and sharpness filters; and overlaying random
stripes of the color of different widths. 

We further divide generative manipulations into three categories:
\textbf{inpainting} where an object is removed from the image,
\textbf{replacement} where an object is replaced with another object, and
\textbf{property change} where the object's material properties are altered. We
illustrate different types of manipulations in Fig. \ref{fig:edit_types}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/edit_types.pdf}
    \caption{Illustration of the different types of manipulations performed using Firefly Generative Fill.
    \textbf{(left)} Inpainting is done by using the word \textit{background} as the prompt. 
    \textbf{(middle)} property change is done by prompting GPT3.5 to output a likely change in color, material, texture or other applicable property of the object.
    \textbf{(right)} replacement is done by prompting GPT3.5 to output a likely replacement candidate object that would be a close match to the shape of the original, but different semantically.
}
    \label{fig:edit_types}
\end{figure*}

Generative manipulations are applied using Firefly Generative Fill\footnote{\url{https://firefly.adobe.com/upload/inpaint}}, which is a language-guided inpainting model. In addition to
the image itself, the model is provided with a segmentation mask and a text
prompt. We generate a convex hull of the segmentation mask and apply dilation to
it to ensure that no part of the object remains outside of the mask.
The origin of the text prompt depends on the type of manipulation. For
\textbf{inpainting} we use the word \textit{background}, which was shown to
perform on par with inpainting-specific models. For
\textbf{replacement}, we further illustrate the image editing pipeline in Fig.~\ref{fig:generation_pipeline}, where we use GPT3.5 in a few-shot learning manner, prompting
with a localized narrative for the whole image, a bounding box of the mask, and
the class label of the mask to come up with a probable replacement candidate
object that would be a close match to the shape of the original object. We use
a similar strategy for \textbf{property change}, but prompting GPT3.5 to
output a likely property change. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/generation_pipeline.pdf}
    \caption{METS image generation pipeline for generative manipulations. 
    The image, its localized narrative, object class name, and segmentation mask are sampled from the OpenImages dataset. 
    The localized narrative and class name are used to construct a prompt for GPT3.5, which outputs a likely replacement candidate object or a property change.
    The prompt templates are manipulation-type specific and can be seen in suppmat. 
    In the case of inpainting, the GPT3.5 block is omitted, and the prompt is simply \textit{background}. The pre-processing of the segmentation mask ensures that no part of the object remains outside of the mask.
    It involves generating a convex hull of the mask and applying dilation to it.
    The generative manipulation is then conditioned on the image, the mask, and the prompt and applied using Firefly Generative Fill.
}
    \label{fig:generation_pipeline}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/sequence.pdf}
    \caption{An example of a sequence of manipulations in METS. 
    The original image is shown in the first column, followed by the manipulated images. 
    The binary masks of the manipulated regions are superimposed on the images.
    The machine annotations generated during the sequence creation are shown in orange, while the human annotations are shown in blue.
    Note that only edit steps 5, 10, and 15 are shown, as these are the steps for which human annotations were collected. 
    All other data types are available for all steps.
    }
    \label{fig:sequence}
\end{figure*}

\subsubsection{Sequence Generation}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/diagram.pdf}
    \caption{The diagram of the sequence generation process. For each image,
         we first go through up to 15 segmentation masks and apply edits, chosen
         randomly, where the probabilities of choices depend on the number
           of edits already applied to the mask. The probability of applying a 
           generative manipulation is greatly lowered if a generative manipulation
               has already been applied. This lowers, but does not eliminate, the
               chance of making destructive or mutually exclusive manipulations.}
    \label{fig:diagram}
\end{figure*}


We sample the images from the OpenImages dataset, making use of the provided
segmentation masks and localized narratives. We choose the images with at
least 5 non-overlapping segmentation masks. We then follow a procedure
illustrated in Fig.~\ref{fig:diagram} to apply a sequence of edits to the image. At each
iteration step, we pick a segmentation mask and either apply a generative or a
pixel-level manipulation to that area of the image or move on to the next mask.
The probability of switching to the next mask is proportional to the number of
manipulations already applied to the mask.

Formally, we define the probabilities of applying a generative manipulation $P_g$,
a pixel-level manipulation $P_p$ and moving on to the next mask $P_n$ as follows:
\begin{equation}
     P_g = g - \frac{n}{2}, \quad P_p = (1 - g) - \frac{n}{2}, \quad P_n = 1 - P_g - P_p,
\end{equation}
where $g = 0.9$ if no generative manipulations have been applied to the mask
yet and $g = 0.1$ otherwise. The value of $n$ is proportional to the number of
manipulations already applied to the mask, defined as follows:
\begin{equation}
     n = \max(0, \frac{40 \times (i - i_{min})}{100}),
\end{equation}
where $i$ is the current step and $i_{min}$ is the minimum number of steps
required to move on to the next mask. We set $i_{min} = 5$.

After each manipulation step, we record the type of manipulation, the parameters
of the manipulation, and the binary mask used to apply the manipulation. This information
is saved in a text format. For pixel-level manipulations, the text format is 
as follows:
\begin{quote}
    \centering
     Object: {obj\_name}, manipulation: {edit\_name}, intensity: {intensity}
\end{quote}
where \texttt{obj\_name} is the name of the object as annotated within the OpenImages dataset,
\texttt{edit\_name} is the manipulation type and \texttt{intensity} is chosen at random from a set of
predefined parameters, individual for each manipulation type. 

For generative manipulations, the text format is as follows:
\begin{quote}
    \centering
     Object: {obj\_name}, replacement: {prompt}
\end{quote}
where \texttt{prompt} is either \textit{background} for inpainting or the output of GPT3.5 for replacement
and property change manipulations. Examples of the template-generated text can be seen from Fig.~\ref{fig:sequence}, marked as \textit{machine annotation}.

As a result, for each input image, we obtain a sequence of manipulated versions
applied on top of each other and a list of annotations describing each
manipulation step type, parameters, and location. We generate 1000 such sequences
with an average of 21.4 steps per sequence.

\subsubsection{Labelling}

We collect human annotations for difference summarization at the 5th, 10th, and 15th
step of the manipulation sequence. In each task, the users are presented with
the input image $I$ and an output image $I'_n, n\in [5,10,15]$ and are asked to
provide a short one-sentence summary of all of the differences they see between the two images.
Examples of such summaries can be seen in Fig. \ref{fig:sequence}, marked as
\textit{human annotations}.

\subsection{Architecture}
\label{sec:arch}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/arch.pdf}
    \caption{Architecture diagram of the model. The LLaMA-2 language model is
         conditioned using the multi-modal instruction template, which includes
         at least two image features and optional auxiliary textual information. All optional content is placed within dashed boxes.
         The image features extracted from the ViT image encoder are concatenated in groups of 4 and projected to the LLM embedding space with a linear projection layer.
    The visual encoder weights are frozen, and only the language model and the projection layer are trained.}
    \label{fig:arch}
    
\end{figure*}



Our architecture is illustrated in Fig.~\ref{fig:arch}. Our
setup consists of a Vision Transformer (ViT) \cite{vit} image encoder and the
open-sourced LLaMA2-chat (7B) large language model \cite{llama2}. The visual
tokens are concatenated in groups of 4 and projected to the language model's
embedding space with a linear projection layer. During training, the visual
encoder weights are frozen, and only the language model and the projection layer
are trained.

Uniquely, we use multiple images as input to the model and train it for the
task of image difference captioning. We note that this approach is capable of
handling an arbitrary number of input images, which allows us to input several
snapshots of the image editing sequence at once.

Optionally, we provide the model with auxiliary textual information in the form
of machine annotations, described in Section \ref{sec:data}. The annotations
for each manipulation are interleaved with the image features and are used to
guide the model's attention to the relevant parts of the image. 

We follow the multi-modal instructional template from \cite{minigpt} and
adjust it to our task:

% quote
\begin{quote}
     \centering
    \tiny
    \textit{[INST] $<$Img$>$$<$ImageFeature$>$$<$/Img$>$ T \ldots $<$Img$>$$<$ImageFeature$>$$<$/Img$>$ T [idc] ins [/INST]}
\end{quote}

where the image feature tags are repeated for each input image in the sequence, 
\texttt{T} is the optional auxiliary textual information,
\texttt{[idc]} is the task identifier for image difference captioning and
\texttt{ins} is the instruction that is chosen at random from a set of
predefined instructions, all synonymous with \textit{describe the defferences between the images.}.

The model is trained to minimize the captioning loss, which is defined as
\begin{equation}
    \mathcal{L} = -\sum^{m}_{i=1}l(s^v, s^{t}_1, \ldots, s^{t}_i),
\end{equation}
where $m$ is a variable token length and $l$ is next-token log-probability conditioned on the previous sequence elements

\begin{equation}
    l(s^v, s^{t}_1, \ldots, s^{t}_i) = \log p(t_i| x, t_1, \ldots, t_{i-1}).
\end{equation}

\subsubsection{Training}
All of the models are trained on a single A100 GPU with 80GB of memory for 
300 epochs with 1000 steps per epoch and a batch size of 6. We use AdamW optimizer with a cosine learning
rate scheduler with an initial learning rate of $10^{-5}$ and a warmup learning rate
of $10^{-6}$ for a warmup period of 1000 steps. The input image size is $448\times448$, and 
the maximum token length is 1024.


\section{Experiments}
\subsection{Datasets}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/other_ds.pdf}
    \caption{Examples of images and annotations from the CLEVR-Change, Spot-the-Diff, MagicBrush and PSBattles datasets.}
    \label{fig:other_ds}
\end{figure*}



In addition to our own dataset, we train and evaluate our model on a number of
other datasets used in the image difference captioning literature illustrated 
in Fig. \ref{fig:other_ds}.

\noindent\textbf{CLEVR-Change} \cite{clevr-change} consists of 67,660, 3,976, 7,970
training, validation, and test image pairs, respectively. The images are generated
using the CLEVR engine and contain renders of primitive 3D shapes. The types of
edits include changes in shape, color, material, size, and position of the objects.
This dataset serves as a good benchmark due to its large volume and precise
annotations. However, the synthetic nature of the images creates a large domain
gap, making it difficult to generalize to real-world images.

\noindent\textbf{Spot-the-Diff} \cite{spot-the-diff} is a dataset of 13,192 well-aligned
image pairs from CCTV cameras. There are no viewpoint changes, and the edits are
limited to object addition, deletion, or movement. The dataset is split into 
training, validation, and test sets following the official split of 80\%, 10\% and 10\%.

\noindent\textbf{PSBattles} \cite{psbattles} is a dataset of real-world image
pairs collected from the Reddit Photoshop Battles subreddit. The difference
captions for a subset of the dataset were collected by \cite{vixen} in a user
study. We use this dataset for the evaluation of the model's generalization
capability to real-world images.

\noindent\textbf{InstructPix2Pix} \cite{ip2p} is a dataset of $\sim$1M image
pairs generated with prompt-to-prompt \cite{p2p} approach. The
difference captions are later generated by \cite{vixen} using chatGPT-3. We use
this dataset for pre-training of the model during the evaluation in the PSBattles
dataset to assess the benefits of fine-tuning on the METS dataset for domain
adaptation.

\noindent\textbf{MagicBrush} \cite{magicbrush} contains sequences of edited images
generated in a manner similar to ours, but with human supervision. Due to the
need for human supervision, the maximum length of the sequences is limited to
3 steps. Of 878 training sequences, only 304 have a length of 4 (including the original image), and 547 have 
a length of 3. We use this dataset to evaluate the model's performance in the
IDC-MI setting, using only the samples that have a length of 4. The target annotation
is a concatenation of the instructions for each step. As input, we use either
the first and the last image in the sequence or all four images in the sequence.

\subsection{Evaluation}

We evaluate the performance of our model in two different settings: standard IDC
with two images as input
and 'image difference captioning with multiple inputs' (IDC-MI). The former
setting is the most common in the literature, while the latter is a novel
setting that we introduce in this work. 

We evaluate the performance of our
model on the standard IDC setting on the CLEVR-Change, InstructPix2Pix, and
PSBattles datasets. We evaluate the performance of our model in the IDC-MI
setting on the MagicBrush and our proposed METS datasets. In both cases, we use
the standard n-gram based metrics BLEU-4 (B4), CIDEr (C),
METEOR (M), ROUGE-L (R) and SPICE (S) to evaluate the performance of our model. 

\subsubsection{Evaluating IDC with Multiple Inputs}

\begin{table}[ht]
\caption{Performance evaluation in the IDC-MI setting shows BLEU-4 (\textbf{B4}), CIDEr (\textbf{C}), METEOR (\textbf{M}) and ROUGE-L (\textbf{R}) scores.
     We report the performance of our model and compare it with GPT3.5 and GPT4-V, varying the number of input images and the presence of auxiliary textual information.}
\label{tab:idc-mi}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lccccccr}
    \toprule
         \textbf{model}&  \textbf{training data}& \textbf{images}& \textbf{text}&  \textbf{B4}&  \textbf{C}&  \textbf{M}& \textbf{R}\\
 % \midrule
 % \multicolumn{6}{c}{Image Edit Requests}\\
 %  \midrule
 % vixen-c& ip2p + IER& 8.6& 38.1& 15.4&42.5\\
 % VARD& IER& 10.0& 35.7& 14.8&39.0\\
 %         minigpt&  IER&  6.5&  28.4&  13.6& 32.3\\
 % minigpt& ip2p + ours + IER& 6.2& 27.2& 14.6&33.1\\
         \midrule
         \multicolumn{8}{c}{METS}\\
         \midrule
         % minigpt& ip2p & 2 & no & 0.0 & 5.6 & 4.9 & 10.6\\
         GPT3.5 \cite{gpt-3}& few-shot & 0 & yes & 1.6 & 8.6 & 10.4 & 15.1\\
         GPT4-V \cite{gpt4}& few-shot & 2 & no & 4.0 & 18.6 & \bf14.0 & 20.3\\
         GPT4-V\cite{gpt4}& few-shot & 2 & yes & 1.3 & 0.3 & 11.5 & 13.5\\
         GPT4-V\cite{gpt4}& few-shot & 4 & no & 3.0 & 15.1 & \underline{13.4} & 19.9\\
         GPT4-V\cite{gpt4}& few-shot & 4 & yes & 1.4 & 0.4 & 11.6 & 12.9\\
         % minigpt& METS & 0 & yes & - & - & - & - \\
         FVTC-2 (ours)& METS & 2 & no & 5.8 & 20.7 & 11.4 & 23.1\\
         FVTC-2T (ours)& METS & 2 & yes & \underline{7.8} & \underline{25.8} & 13.0 & \underline{26.0}\\
         FVTC-4 (ours)& METS & 4 & no  & 6.6 & 23.5 & 12.3 & 24.3\\
         FVTC-4T (ours)& METS & 4 & yes & \bf 8.2 & \bf 25.9 & \underline{13.4} & \bf 26.3\\
         % \midrule
         % \multicolumn{6}{c}{Instruct Pix2Pix Reverse}\\
         % \midrule
         % vixen-c& ip2p& \bf11.9& \bf46.5& 13.3 & \bf33.6\\
         % minigpt& ip2p& 11.5& 43.4& \bf14.0 & 28.6\\
         % \midrule
         % \multicolumn{6}{c}{Instruct Pix2Pix Average}\\
         % \midrule
         % vixen-c& ip2p& \bf14.4& \bf72.4& 15.5 & \bf33.8&\\
         % minigpt& ip2p& 13.3& 61.7& \bf16.0 & 31.1\\
         \midrule
         \multicolumn{8}{c}{MagicBrush}\\
         \midrule
         FVTC-2 (ours)& MagicBrush & 2 & no & 4.9 & 29.4 & 13.3 & 28.1\\
         FVTC-4 (ours)& MagicBrush & 4 & no & \bf 6.8 & \bf 44.5 & \bf 15.6 & \bf 31.2\\
         \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

For the IDC-MI setting, we evaluate the model's performance while varying the
number of input images and the presence of auxiliary textual information. The
intermediate images are sampled to be equally spaced in the sequence, and the
textual information is provided in the form of  machine annotations described
in Section \ref{sec:data}.
We compare the performance of our model with
GPT4-V, which has multi-modal capabilities and is capable of taking multiple
images and/or text as input. Additionally, we compare with GPT3.5, which serves as a text-only baseline, taking as input only the auxiliary text and no images.

The results of the IDC-MI setting are shown in Table \ref{tab:idc-mi}. We demonstrate
that our method is able to take advantage of the additional inputs, achieving the best
performance when both intermediate images and auxiliary textual information are present.
On the other hand, GPT4-V suffers from the addition of intermediate images and text,
showing a decrease in performance in both cases.

Compared to the base case of just two-image input, the addition of text to our model improves the performance
by an average of 18.9\% across all metrics, and intermediate images improve the performance
by an average of 10.1\% across all metrics. The combination of both intermediate images and
textual information shows an average improvement of 22.4\% across all metrics. 

On the other hand, the performance of GPT4-V suffers from the addition of intermediate images, 
decreasing in performance with the addition of both extra images and text. Inference examples in
Fig. \ref{fig:inference} shows that GPT4-V gets confused by the presence of
multiple images, referencing them directly in the captions.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/inference.pdf}
    \caption{Examples of model inference on METS (top 4), CLEVR-Change (bot-left), and Spot-the-Diff (bot-right) datasets.
    FVTC-2 and FVTC-4 denote the model trained on METS with 2 and 4 input images, respectively. "T" denotes the presence of auxiliary textual information.}
    \label{fig:inference}
\end{figure}

\begin{table}[ht]
\caption{Image difference captioning performance evaluation in the standard IDC setting on the CLEVR-Change and PSBattles datasets.
We report the performance of our model and compare it with the state-of-the-art models and report 
BLEU-4 (\textbf{B4}), CIDEr (\textbf{C}), METEOR (\textbf{M}) and ROUGE-L (\textbf{R}) scores.}
\label{tab:idc}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{lccccccr}
         \toprule
         \textbf{model}&  {training data}&  \textbf{B4}&  \textbf{C}&  \textbf{M}& \textbf{R}  & \textbf{S}\\
         % \midrule
         % \multicolumn{6}{c}{Instruct Pix2Pix}\\
         % \midrule
         % vixen-c& ip2p& \bf16.9& \bf98.2& 17.6&\bf39.4\\
         % minigpt& ip2p& 15.0 & 80.0 & \bf18.1 & 33.6\\
         \midrule
         \multicolumn{6}{c}{CLEVR Change}\\
         \midrule
         DUDA \cite{DUDA}& CLEVR & 47.3 & 112.3 & 33.9 & - & -\\
         IFDC \cite{IFDC} & CLEVR & 49.2 & 118.7 & 32.5 & 69.1& -\\
         $R^3$Net+SSP \cite{R3Net} & CLEVR & 54.7 & 123.0 & 39.8 & 73.1& -\\
         SGCC \cite{SGCC} & CLEVR & \underline{51.1} & 121.8 & \underline{40.6} & 73.9& -\\
         NCT \cite{NCT} & CLEVR & \underline{55.1} & \underline{124.1} & 40.2 & 73.8& -\\
         SRDL+AVS \cite{sem}& CLEVR & 54.9 & 122.2 & 40.2 & 73.3& -\\
         VARD \cite{VARD}& CLEVR & \bf 55.2& \underline{124.1} & \bf 40.8 & \underline{74.1}& - \\
         FVTC-2 (ours) & CLEVR & 54.7 & \bf 151.8 & 40.0 & \bf 77.1& -\\
         \midrule
         \multicolumn{6}{c}{Spot-the-Diff}\\
         \midrule
         SRDL+AVS \cite{sem}& Spot-Diff & - & 35.3 & 13.0 & 31.0 & 18.0\\
         $R^3$Net+SSP \cite{R3Net} & Spot-Diff & - & 36.6 & \underline{13.1} & \bf 32.6 & \underline{18.8} \\
         VARD-LSTM \cite{VARD}& Spot-Diff & - & \underline{39.3} & \underline{13.1} & \underline{33.1} & 17.5\\
         VARD-Transformer \cite{VARD} & Spot-Diff & - & 30.3 & 12.5 & 29.3 & 17.3\\
         FVTC-2 (ours) & Spot-Diff & - & \bf 45.5 & \bf 13.7 & 28.7 & \bf 19.3\\
         \midrule
         \multicolumn{6}{c}{PSBattles}\\
         \midrule
         vixen-c \cite{vixen}& ip2p& 4.5& 7.7& 9.5&20.5& -\\
         FVTC-2 (ours)& ip2p& 5.3& 10.3& 10.8&22.& -\\
         FVTC-2 (ours)& ip2p + METS & \bf 5.5& \bf 14.2& \bf 11.2& \bf 22.6& -\\
         \bottomrule

    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsubsection{Evaluating IDC with Two Inputs}


We observe that in the IDC setting, shown in Table \ref{tab:idc}, the model achieves competitive performance on the CLEVR-Change dataset,
outperforming the previous state-of-the-art model VARD on the CIDEr and ROUGE-L metrics.
On the InstructPix2Pix dataset, the model outperforms VIXEN only on the METEOR metric.
However, it shows a better capability to generalize to real-world images, outperforming
VIXEN on the PSBattles dataset for all metrics. Additionally, fine-tuning the model
on the METS dataset further improves its performance on PSBattles, showing the dataset's
ability to bridge the domain gap between synthetic and real-world images.


\subsection{Limitations}

As with most LLMs, FVTC can occasionally hallucinate details that are not
present in the input images. Additionally, we note several occurrences of miscounting: saying two objects were modified, when it should be just one and vice versa. The examples of partially incorrect captions are included in supmat.



\section{Conclusion}

We have introduced a novel task of image difference captioning with multiple
inputs and demonstrated that the presence of additional visual and/or textual
inputs improves the model's captioning performance. We have introduced METS -- a
new dataset of long image editing sequences paired with machine annotations and
human edit summarization captions. We have trained a multi-modal LLM with
multiple visual inputs and provided a comprehensive evaluation of the benefits
of both additional visual and textual inputs. Additionally, we have demonstrated
that fine-tuning a model that is trained on other synthetic data with METS helps
to bridge the domain gap and improves zero-shot performance on real-life images.

% The exploration of the model's confidence and self-awareness with regard to the
% hallucinations, similar to \cite{h1, h2} could be an interesting
% direction for future work.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
